import tensorflow as tf
import tensorflow_probability as tfp

tfd = tfp.distributions

from softlearning.utils.keras import PicklableKerasModel,NormRegularizer

SCALE_DIAG_MIN_MAX = (-20, 2)

def sampling(args):
    z_log_var = args
    batch = tf.keras.backend.shape(z_log_var)[0]
    dim = tf.keras.backend.int_shape(z_log_var)[1]
    # by default, random_normal has mean=0 and std=1.0
    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
    return tf.keras.backend.exp(0.5 * z_log_var) * epsilon


def feedforward_model(input_shapes,
                      output_size,
                      hidden_layer_sizes,
                      activation='relu',
                      output_activation='linear',
                      preprocessors=None,
                      name='feedforward_model',
                      *args,
                      **kwargs):
    inputs = [
        tf.keras.layers.Input(shape=input_shape)
        for input_shape in input_shapes
    ]

    if preprocessors is None:
        preprocessors = (None, ) * len(inputs)

    preprocessed_inputs = [
        preprocessor(input_) if preprocessor is not None else input_
        for preprocessor, input_ in zip(preprocessors, inputs)
    ]

    concatenated = tf.keras.layers.Lambda(
        lambda x: tf.concat(x, axis=-1)
    )(preprocessed_inputs)

    out = concatenated
    for units in hidden_layer_sizes:
        out = tf.keras.layers.Dense(
            units, *args, activation=activation, **kwargs
        )(out)

    out = tf.keras.layers.Dense(
        output_size, *args, activation=output_activation, **kwargs
    )(out)

    #print(out)

    model = PicklableKerasModel(inputs, out, name=name)
    model.summary()
    return model

